{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "All tasks are words counting. It is find the main challenge is pre-processing the raw data into normal and clean form.\n",
    "\n",
    "1. Spark Session:<br\\>\n",
    "sparkSession will give a context as well called \"sparkSession.sparkContext\". The default sparkSession is \"spark\", and sparkContext is \"sc\".<br\\><br\\>\n",
    "2. Read data:<br\\>\n",
    "txt file:<br\\>\n",
    "```\n",
    "val textRDD = sparkSession.sparkContext.textFile(\"files/TaleOfTwoCities.txt\")\n",
    "val textDS = sparkSession.read.textFile(\"C:/Users/Matthew/Desktop/TaleOfTwoCities.txt\")\n",
    "val textDF = sparkSession.read.text(\"C:/Users/Matthew/Desktop/TaleOfTwoCities.txt\")\n",
    "val raw1RDD = sparkSession.sparkContext.textFile(\"files/docword.enron.txt.gz\") // only one partition\n",
    "```\n",
    "csv file:<br\\>\n",
    "```\n",
    "val raw1RDD = sparkSession.sparkContext.textFile(\"files/CensusIncomeData.csv\", 1)\n",
    "val raw1DF = sparkSession.read.\n",
    "    format(\"csv\").\n",
    "    option(\"header\", \"false\").\n",
    "    load(\"files/Data.csv\") // only one partition\n",
    "```\n",
    "3. Access into first element of data and check length:<br\\>\n",
    "List or Seq: a(0), a.length<br\\>\n",
    "Array: a(0), a.length<br\\>\n",
    "Tuple: a.\\_1<br\\>\n",
    "Row: a.get(0) or a(0) (Type Any)<br\\>\n",
    "    a.getDouble(0) (Type Double)<br\\>\n",
    "    a(0).asInstanceOf\\[Double\\] (Type Double)<br\\>\n",
    "    a.length<br\\><br\\>\n",
    "4. Check number of partitions:<br\\>\n",
    "```\n",
    "dataRDD.partitions.size\n",
    "dataDF.rdd.partitions.size\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://143.167.112.136:4042\n",
       "SparkContext available as 'sc' (version = 2.2.0, master = local[*], app id = local-1522079562541)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover}\n",
       "import org.apache.spark.sql.Row\n",
       "sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6da3598d\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover}\n",
    "import org.apache.spark.sql.Row\n",
    "val sparkSession = SparkSession.builder.\n",
    "    master(\"local[4]\").\n",
    "    appName(\"Text Processing\").\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: pipeline\n",
    "1. Read txt file into RDD.\n",
    "2. Use \"pipeline\" to change the capital words into small, and remove the stop words.\n",
    "3. Use \"explode\" to remove the empty row in DataFrame.\n",
    "4. Use \"withColumn\" to append a column, use \"agg\" to append a column with \"groupBy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawRDD: org.apache.spark.rdd.RDD[String] = files/AnimalFarmChap1.txt MapPartitionsRDD[1] at textFile at <console>:31\n",
       "wordsDF: org.apache.spark.sql.DataFrame = [words: string]\n",
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_ed1e9b186e09\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_d37722f00621\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_f022bf73670a\n",
       "model: org.apache.spark.ml.PipelineModel = pipeline_f022bf73670a\n",
       "pipedDF: org.apache.spark.sql.DataFrame = [words: string, smallWords: array<string> ... 1 more field]\n",
       "dataDF: org.apache.spark.sql.DataFrame = [words: string, smallWords: array<string> ... 2 more fields]\n",
       "mostFreqDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [explodedWords: string, TopFreqWords: bigint]\n",
       "words6DF: org.apache.spark.s..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read data\n",
    "val rawRDD = sparkSession.sparkContext.textFile(\"files/AnimalFarmChap1.txt\", 20)\n",
    "val wordsDF = rawRDD.flatMap(_.split(\"\\\\W\").filter(_!=\"\")).toDF(\"words\")\n",
    "\n",
    "// Build pipeline\n",
    "val tokenizer = new Tokenizer().\n",
    "    setInputCol(\"words\").\n",
    "    setOutputCol(\"smallWords\")\n",
    "\n",
    "val remover = new StopWordsRemover().\n",
    "    setInputCol(\"smallWords\").\n",
    "    setOutputCol(\"smallNoStopWords\")\n",
    "\n",
    "val pipeline = new Pipeline().\n",
    "    setStages(Array(tokenizer, remover))\n",
    "\n",
    "val model = pipeline.fit(wordsDF)\n",
    "val pipedDF = model.transform(wordsDF)\n",
    "\n",
    "// explode used for flatten data. But here, we used it to remove empty cell\n",
    "val dataDF = pipedDF.withColumn(\"explodedWords\", explode($\"smallNoStopWords\")) // \"withColumn\" is used to add column, so do agg. But \"agg\" used with \"groupBy\"\n",
    "val mostFreqDF = dataDF.groupBy(\"explodedWords\").agg(count(\"*\") as \"TopFreqWords\").orderBy(desc(\"TopFreqWords\")) // \"count\" can automatically add a column named by count\n",
    "\n",
    "// Count words which is at least 6 characters\n",
    "val words6DF = rawRDD.flatMap(_.split(\"\\\\W\").filter(_!=\"\").filter(_.length >= 6)).toDF(\"words\")\n",
    "val piped6DF = model.transform(words6DF) // model do not need to retrain, as there is only transformer in pipeline\n",
    "val data6DF = piped6DF.withColumn(\"explodedWords\", explode($\"smallNoStopWords\"))\n",
    "val mostFreq6DF = data6DF.groupBy(\"explodedWords\").agg(count(\"*\") as \"TopFreqWords\").orderBy(\"TopFreqWords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "1. Read csv file into RDD\n",
    "2. Remove the last incomplete element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| degree|count|\n",
      "+-------+-----+\n",
      "|Masters| 1341|\n",
      "+-------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "raw1RDD: org.apache.spark.rdd.RDD[String] = files/CensusIncomeData.csv MapPartitionsRDD[5] at textFile at <console>:39\n",
       "fileEnd: Int = 25776\n",
       "rawRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at repartition at <console>:41\n",
       "dataRDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[11] at map at <console>:42\n",
       "dataDF: org.apache.spark.sql.DataFrame = [degree: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raw1RDD = sparkSession.sparkContext.textFile(\"files/CensusIncomeData.csv\", 1)\n",
    "val fileEnd = raw1RDD.count.toInt // convert long into int for arithmetic operation\n",
    "val rawRDD = raw1RDD.mapPartitions(_.take(fileEnd-1)).repartition(20) // remove the last empty line.\n",
    "val dataRDD = rawRDD.map(_.split(\",\").map(_.trim))\n",
    "val dataDF = dataRDD.map(_(3)).toDF(\"degree\")\n",
    "dataDF.groupBy(\"degree\").count.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3:\n",
    "1. Read csv into DataFrame directly. Only 1 partition.\n",
    "2. Get the element in Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|     _c3|count|\n",
      "+--------+-----+\n",
      "|    10th|  753|\n",
      "| Masters| 1341|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "raw1DF: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 13 more fields]\n",
       "dataDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_c3: string]\n",
       "sudDataDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_c3: string]\n",
       "countDF: org.apache.spark.sql.DataFrame = [_c3: string, count: bigint]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read data direct into DataFrame. This way avoid error \"ArrayIndexOutOfBoundsException\" caused by the last row do not contain \"degree\".\n",
    "// The \"degree\" in the last line is filled with \"null\".\n",
    "val raw1DF = sparkSession.read.format(\"csv\").\n",
    "    option(\"header\", \"false\").\n",
    "    load(\"files/CensusIncomeData.csv\")\n",
    "val dataDF = raw1DF.select(\"_c3\").repartition(20)\n",
    "val sudDataDF = dataDF.filter(x => (x.get(0)==\" Masters\") || (x.get(0)==\" 10th\")) // x is Row, use \"get\" to obtaint the element in Row.\n",
    "val countDF = sudDataDF.groupBy(\"_c3\").count\n",
    "countDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "1. Read gz file. Only one partition.\n",
    "2. Convert RDD into tuple type, and the number of elements in tuple will be the number of DF columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|wordID|count|\n",
      "+------+-----+\n",
      "| 18130|  795|\n",
      "| 14204|  802|\n",
      "| 26005|   71|\n",
      "| 23843| 1505|\n",
      "| 17686| 2619|\n",
      "| 23097|   52|\n",
      "| 12394|  138|\n",
      "| 11888|  422|\n",
      "| 24269|  494|\n",
      "| 14369|   23|\n",
      "| 20569|  607|\n",
      "| 21452|   83|\n",
      "| 14157|  229|\n",
      "| 25555|  645|\n",
      "| 14887|  877|\n",
      "| 23459|   77|\n",
      "|  1159|  430|\n",
      "| 25032|  250|\n",
      "|   467|  174|\n",
      "| 18726|  138|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "raw1RDD: org.apache.spark.rdd.RDD[String] = files/docword.enron.txt.gz MapPartitionsRDD[37] at textFile at <console>:38\n",
       "rawRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[42] at repartition at <console>:39\n",
       "dataRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[44] at map at <console>:40\n",
       "dataDF: org.apache.spark.sql.DataFrame = [wordID: string, count: string]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raw1RDD = sparkSession.sparkContext.textFile(\"files/docword.enron.txt.gz\", 1)\n",
    "val rawRDD = raw1RDD.mapPartitions(_.drop(3)).repartition(20)\n",
    "val dataRDD = rawRDD.map(_.split(\"\\\\W\")).map(x => (x(1), x(2))) // to converted into two columns DataFrame, we need tuple whose first element is accessed by x._1\n",
    "val dataDF = dataRDD.toDF(\"wordID\", \"count\")\n",
    "dataDF.groupBy(\"wordID\").count.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
