{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "1. Decision tree classifier\n",
    "2. Decision tree regression <br/>\n",
    "\n",
    "The evaluatiors are \"BinaryClassificationEvaluator\", \"RegressionEvaluator\", and \"MulticlassClassificationEvaluator\". All of them take two inputs (label, and prediction), and one output (metric). The model prediction performance is evluated by the performance indicator (metric). See detail:<br/>\n",
    "Binary:https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html#metricName--\n",
    "Regreesion:https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/ml/evaluation/RegressionEvaluator.html#metricName--\n",
    "Multiclass:https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/ml/evaluation/MulticlassClassificationEvaluator.html#metricName--<br/><br/>\n",
    "Supported impurity for classfier and regressor<br/>\n",
    "Classifier:https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/ml/classification/DecisionTreeClassifier.html#supportedImpurities--\n",
    "Regressor:https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/ml/regression/DecisionTreeRegressor.html#supportedImpurities--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.regression.{DecisionTreeRegressor, DecisionTreeRegressionModel}\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "import org.apache.spark.ml.classification.{DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
       "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, RegressionEvaluator}\n",
       "import org.apache.spark.sql.Row\n",
       "sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@69954667\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.regression.{DecisionTreeRegressor, DecisionTreeRegressionModel}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.classification.{DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, RegressionEvaluator}\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val sparkSession = SparkSession.builder.\n",
    "    master(\"local[4]\").\n",
    "    appName(\"Decision Tree\").\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Decision Tree 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC on the test data is 0.6484120120797818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawRDD: org.apache.spark.rdd.RDD[String] = files/credit_card_clients.csv MapPartitionsRDD[1] at textFile at <console>:35\n",
       "fileEnd: Int = 30003\n",
       "noHeaderRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at mapPartitions at <console>:37\n",
       "dataRDD: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[8] at repartition at <console>:38\n",
       "dataDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n",
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "labelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_fbffeb218290\n",
       "featureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_c4f7c9863390\n",
       "dt:..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read Data\n",
    "val rawRDD = sparkSession.sparkContext.textFile(\"files/credit_card_clients.csv\", 1)\n",
    "val fileEnd = rawRDD.count.toInt // convert long into int for arithmetic operation\n",
    "val noHeaderRDD = rawRDD.mapPartitions(_.take(fileEnd-1)).mapPartitions(_.drop(2)) // remove the last empty line and the first two lines (header)\n",
    "val dataRDD = noHeaderRDD.map(_.replace(\"\\\"\", \"\").split(\",\").map(_.toDouble)).repartition(20)\n",
    "//*dataRDD.partitions.size // check partition size\n",
    "//*dataRDD.groupBy(\"label\").count.show // check the labels\n",
    "val dataDF = dataRDD.map(x => (x(24), Vectors.dense(x.take(24)))).toDF(\"label\", \"features\")\n",
    "val Array(trainingData, testData) = dataDF.randomSplit(Array(0.7, 0.3), seed = 123)\n",
    "\n",
    "// Build pipeline about decision tree classification\n",
    "val labelIndexer = new StringIndexer().\n",
    "  setInputCol(\"label\").\n",
    "  setOutputCol(\"indexedLabel\").\n",
    "  fit(dataDF)\n",
    "\n",
    "val featureIndexer = new VectorIndexer().\n",
    "  setInputCol(\"features\").\n",
    "  setOutputCol(\"indexedFeatures\").\n",
    "  setMaxCategories(10). // features with > 10 distinct values are treated as continuous.\n",
    "  fit(dataDF)\n",
    "\n",
    "val dt = new DecisionTreeClassifier().\n",
    "  setLabelCol(\"indexedLabel\").\n",
    "  setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "val labelConverter = new IndexToString().\n",
    "  setInputCol(\"prediction\").\n",
    "  setOutputCol(\"predictedLabel\").\n",
    "  setLabels(labelIndexer.labels)\n",
    "\n",
    "val pipeline = new Pipeline().\n",
    "  setStages(Array(labelIndexer, featureIndexer, dt, labelConverter))\n",
    "\n",
    "// Cross validation\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "  addGrid(dt.maxBins, Array(16, 32, 64)).\n",
    "  addGrid(dt.maxDepth, Array(5, 7, 9)).\n",
    "  addGrid(dt.impurity, Array(\"gini\", \"entropy\")).\n",
    "  build()\n",
    "\n",
    "val cv = new CrossValidator().\n",
    "  setEstimator(pipeline).\n",
    "  setEvaluator(new BinaryClassificationEvaluator).\n",
    "  setEstimatorParamMaps(paramGrid).\n",
    "  setNumFolds(5)  // Use 3+ in practice\n",
    "\n",
    "val cvModel = cv.fit(trainingData)\n",
    "\n",
    "// Evaluation\n",
    "val evaluator = new BinaryClassificationEvaluator().\n",
    "    setLabelCol(\"indexedLabel\").\n",
    "    setRawPredictionCol(\"prediction\").\n",
    "    setMetricName(\"areaUnderROC\")\n",
    "\n",
    "val predictions = cvModel.transform(testData)\n",
    "// Check the prediction\n",
    "// predictions.filter(_.getDouble(0) > 0.5).count\n",
    "// predictions.filter(_(5).asInstanceOf[Vector](0) > 0.5).count\n",
    "// Note: both \"Vector\" and \"Vectors\" are needed to be imported, i.e.\n",
    "// import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "// Vector for the data type in \"asInstanceOf[]\"\n",
    "// Vectors for creating a vector in \"Vectors.dense()\"\n",
    "\n",
    "val roc = evaluator.evaluate(predictions)\n",
    "println(\"The ROC on the test data is \" + roc)\n",
    "\n",
    "// Check the model property\n",
    "val bestDTM = cvModel.bestModel.asInstanceOf[PipelineModel].stages(2).asInstanceOf[DecisionTreeClassificationModel]\n",
    "//*bestDTM.getImpurity\n",
    "//*bestDTM.getMaxBins\n",
    "//*bestDTM.getMaxDepth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Decision Tree 2\n",
    "1. Drop lines containing <tt>NaN</tt>\n",
    "2. Replace <tt>NaN</tt> with the average value from that column<br><br>\n",
    "\n",
    "### Read data with 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Read data Drop lines containing NaN\n",
    "val rawRDD = sparkSession.sparkContext.textFile(\"files/subject101.dat\")\n",
    "val dataRDD = rawRDD.filter(!_.contains(\"NaN\")).map(_.split(\" \").map(_.toDouble)).repartition(20) // \"!\" operator can not used in \"!a.contains()\"\n",
    "val dataDF = dataRDD.map(x => (x(1), Vectors.dense(x.take(54).drop(2)))).toDF(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or read data with 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Replace NaN with the average value from that column\n",
    "val rawRDD = sparkSession.sparkContext.textFile(\"files/subject101.dat\")\n",
    "val rawDF = rawRDD.repartition(20).toDF\n",
    "// val auxStr = (1 to 54).map(x => \"$\\\"_tmp\\\"(\" + (x-1).toString + \").as(\" + \"\\\"col\" + x.toString + \"\\\")\")\n",
    "// println(auxStr)\n",
    "val dataNaNDF = rawDF.withColumn(\"_tmp\", split($\"value\", \" \")).select(\n",
    "  $\"_tmp\"(0).as(\"col1\"), $\"_tmp\"(1).as(\"col2\"), $\"_tmp\"(2).as(\"col3\"), $\"_tmp\"(3).as(\"col4\"), $\"_tmp\"(4).as(\"col5\"), $\"_tmp\"(5).as(\"col6\"), $\"_tmp\"(6).as(\"col7\"), $\"_tmp\"(7).as(\"col8\"), $\"_tmp\"(8).as(\"col9\"), $\"_tmp\"(9).as(\"col10\"), $\"_tmp\"(10).as(\"col11\"), $\"_tmp\"(11).as(\"col12\"), $\"_tmp\"(12).as(\"col13\"), $\"_tmp\"(13).as(\"col14\"), $\"_tmp\"(14).as(\"col15\"), $\"_tmp\"(15).as(\"col16\"), $\"_tmp\"(16).as(\"col17\"), $\"_tmp\"(17).as(\"col18\"), $\"_tmp\"(18).as(\"col19\"), $\"_tmp\"(19).as(\"col20\"), $\"_tmp\"(20).as(\"col21\"), $\"_tmp\"(21).as(\"col22\"), $\"_tmp\"(22).as(\"col23\"), $\"_tmp\"(23).as(\"col24\"), $\"_tmp\"(24).as(\"col25\"), $\"_tmp\"(25).as(\"col26\"), $\"_tmp\"(26).as(\"col27\"), $\"_tmp\"(27).as(\"col28\"), $\"_tmp\"(28).as(\"col29\"), $\"_tmp\"(29).as(\"col30\"), $\"_tmp\"(30).as(\"col31\"), $\"_tmp\"(31).as(\"col32\"), $\"_tmp\"(32).as(\"col33\"), $\"_tmp\"(33).as(\"col34\"), $\"_tmp\"(34).as(\"col35\"), $\"_tmp\"(35).as(\"col36\"), $\"_tmp\"(36).as(\"col37\"), $\"_tmp\"(37).as(\"col38\"), $\"_tmp\"(38).as(\"col39\"), $\"_tmp\"(39).as(\"col40\"), $\"_tmp\"(40).as(\"col41\"), $\"_tmp\"(41).as(\"col42\"), $\"_tmp\"(42).as(\"col43\"), $\"_tmp\"(43).as(\"col44\"), $\"_tmp\"(44).as(\"col45\"), $\"_tmp\"(45).as(\"col46\"), $\"_tmp\"(46).as(\"col47\"), $\"_tmp\"(47).as(\"col48\"), $\"_tmp\"(48).as(\"col49\"), $\"_tmp\"(49).as(\"col50\"), $\"_tmp\"(50).as(\"col51\"), $\"_tmp\"(51).as(\"col52\"), $\"_tmp\"(52).as(\"col53\"), $\"_tmp\"(53).as(\"col54\")\n",
    "  ).drop(\"_tmp\")\n",
    "val colLen = dataNaNDF.columns.length\n",
    "val nonNaN = (0 until colLen).map(i => dataNaNDF.select(dataNaNDF.columns(i)).filter(x => x.getAs[String](0) != \"NaN\"))\n",
    "val colMean = (0 until colLen).map(i => nonNaN(i).rdd.map(_(0).toString.toDouble).mean)\n",
    "// Compute mean value without NaN\n",
    "// val data = Seq((1,2,3), (3,4,5), (1,2,4)).toDF(\"A\", \"B\", \"C\")\n",
    "// data.select(data.columns.map(mean(_)): _*).show() // \":_*\" means unpack a Array\n",
    "\n",
    "// Creat column objectives\n",
    "val columnsNaNReplace = dataNaNDF.columns.map(col).map(colName => {\n",
    "    val colNum = colName.toString.drop(3).toInt-1;\n",
    "    when(colName.isNaN,colMean(colNum).toString).otherwise(colName).as(s\"${colName}\")\n",
    "})\n",
    "val allDataDF = dataNaNDF.select(columnsNaNReplace: _*)\n",
    "\n",
    "val dataDF = allDataDF.rdd.map(y => {\n",
    "    val a = y.toSeq.map(x => x.toString.toDouble).toArray; \n",
    "    (a(1), Vectors.dense(a.take(54).drop(2)))}).\n",
    "    toDF(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array(trainingData, testData) = dataDF.randomSplit(Array(0.7, 0.3), seed = 123)\n",
    "\n",
    "// Build pipeline about decision tree classification\n",
    "val featureIndexer = new VectorIndexer().\n",
    "  setInputCol(\"features\").\n",
    "  setOutputCol(\"indexedFeatures\").\n",
    "  setMaxCategories(10). // features with > 10 distinct values are treated as continuous.\n",
    "  fit(dataDF)\n",
    "\n",
    "val dt = new DecisionTreeClassifier().\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "val pl = new Pipeline().\n",
    "  setStages(Array(featureIndexer, dt))\n",
    "\n",
    "val dtModel = pl.fit(trainingData)\n",
    "val predictions = dtModel.transform(testData)\n",
    "\n",
    "val testErr = predictions.select(\"label\", \"prediction\").\n",
    "            filter(r => r.getDouble(0) != r.getDouble(1)).\n",
    "            count().\n",
    "            toDouble / testData.count()\n",
    "println(\"Test Error = \" + testErr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Decision Tree 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the test data is 0.7285336650923595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawRDD: org.apache.spark.rdd.RDD[String] = files/winequality-white.csv MapPartitionsRDD[4899] at textFile at <console>:53\n",
       "noHeaderRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4900] at mapPartitions at <console>:54\n",
       "dataRDD: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[4905] at repartition at <console>:55\n",
       "dataDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n",
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "featureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_4efdb9ef68aa\n",
       "dt: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_73cad0b094ca\n",
       "pipeline: org.apache.spa..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read data\n",
    "val rawRDD = sparkSession.sparkContext.textFile(\"files/winequality-white.csv\", 1)\n",
    "val noHeaderRDD = rawRDD.mapPartitions(_.drop(1)) // remove first line (header)\n",
    "val dataRDD = noHeaderRDD.map(_.split(\";\").map(_.toDouble)).repartition(20)\n",
    "//*dataRDD.partitions.size // check partition size\n",
    "//*dataRDD.groupBy(\"label\").count.show // check the labels\n",
    "val dataDF = dataRDD.map(x => (x(11), Vectors.dense(x.take(11)))).toDF(\"label\", \"features\")\n",
    "val Array(trainingData, testData) = dataDF.randomSplit(Array(0.7, 0.3), seed = 123)\n",
    "\n",
    "// Build pipeline about decision tree regression\n",
    "val featureIndexer = new VectorIndexer().\n",
    "  setInputCol(\"features\").\n",
    "  setOutputCol(\"indexedFeatures\").\n",
    "  setMaxCategories(4). //we treat features with > 4 distinct values as continuous.\n",
    "  fit(dataDF)\n",
    "\n",
    "val dt = new DecisionTreeRegressor().\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "val pipeline = new Pipeline().\n",
    "  setStages(Array(featureIndexer, dt))\n",
    "\n",
    "// Cross validation\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "  addGrid(dt.maxBins, Array(10, 15, 20)).\n",
    "  addGrid(dt.maxDepth, Array(5, 7, 9)).\n",
    "  build()\n",
    "\n",
    "val cv = new CrossValidator().\n",
    "  setEstimator(pipeline).\n",
    "  setEvaluator(new RegressionEvaluator).\n",
    "  setEstimatorParamMaps(paramGrid).\n",
    "  setNumFolds(5)  // Use 3+ in practice\n",
    "\n",
    "val cvModel = cv.fit(trainingData)\n",
    "\n",
    "// Evaluation\n",
    "val evaluator = new RegressionEvaluator().\n",
    "    setLabelCol(\"label\").\n",
    "    setPredictionCol(\"prediction\").\n",
    "    setMetricName(\"rmse\")\n",
    "\n",
    "val predictions = cvModel.transform(testData)\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(\"The RMSE on the test data is \" + rmse)\n",
    "\n",
    "// Check the model property\n",
    "val bestDTM = cvModel.bestModel.asInstanceOf[PipelineModel].stages(1).asInstanceOf[DecisionTreeRegressionModel]\n",
    "//*bestDTM.getImpurity\n",
    "//*bestDTM.getMaxBins\n",
    "//*bestDTM.getMaxDepth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Decision Tree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error = 0.07879840636625587\n",
      "Learned regression tree model:\n",
      "DecisionTreeRegressionModel (uid=dtr_b605f8eb0a48) of depth 5 with 51 nodes\n",
      "  If (feature 51 <= 0.059)\n",
      "   If (feature 6 <= 0.0)\n",
      "    If (feature 23 <= 0.0)\n",
      "     If (feature 15 <= 0.19)\n",
      "      If (feature 52 <= 0.184)\n",
      "       Predict: 0.049800796812749\n",
      "      Else (feature 52 > 0.184)\n",
      "       Predict: 0.5\n",
      "     Else (feature 15 > 0.19)\n",
      "      If (feature 4 <= 1.07)\n",
      "       Predict: 0.22784810126582278\n",
      "      Else (feature 4 > 1.07)\n",
      "       Predict: 0.8181818181818182\n",
      "    Else (feature 23 > 0.0)\n",
      "     If (feature 52 <= 0.052)\n",
      "      If (feature 54 <= 2.63)\n",
      "       Predict: 0.2\n",
      "      Else (feature 54 > 2.63)\n",
      "       Predict: 0.8571428571428571\n",
      "     Else (feature 52 > 0.052)\n",
      "      If (feature 9 <= 0.21)\n",
      "       Predict: 0.9411764705882353\n",
      "      Else (feature 9 > 0.21)\n",
      "       Predict: 0.25\n",
      "   Else (feature 6 > 0.0)\n",
      "    If (feature 26 <= 0.0)\n",
      "     If (feature 45 <= 0.0)\n",
      "      If (feature 49 <= 0.472)\n",
      "       Predict: 0.9387755102040817\n",
      "      Else (feature 49 > 0.472)\n",
      "       Predict: 0.0\n",
      "     Else (feature 45 > 0.0)\n",
      "      Predict: 0.0\n",
      "    Else (feature 26 > 0.0)\n",
      "     Predict: 0.0\n",
      "  Else (feature 51 > 0.059)\n",
      "   If (feature 55 <= 18.0)\n",
      "    If (feature 15 <= 0.07)\n",
      "     If (feature 6 <= 0.0)\n",
      "      If (feature 52 <= 0.082)\n",
      "       Predict: 0.17295597484276728\n",
      "      Else (feature 52 > 0.082)\n",
      "       Predict: 0.7272727272727273\n",
      "     Else (feature 6 > 0.0)\n",
      "      If (feature 20 <= 0.24)\n",
      "       Predict: 0.625\n",
      "      Else (feature 20 > 0.24)\n",
      "       Predict: 1.0\n",
      "    Else (feature 15 > 0.07)\n",
      "     If (feature 26 <= 0.0)\n",
      "      If (feature 45 <= 0.0)\n",
      "       Predict: 0.8712121212121212\n",
      "      Else (feature 45 > 0.0)\n",
      "       Predict: 0.3333333333333333\n",
      "     Else (feature 26 > 0.0)\n",
      "      Predict: 0.0\n",
      "   Else (feature 55 > 18.0)\n",
      "    If (feature 24 <= 0.09)\n",
      "     If (feature 45 <= 0.12)\n",
      "      If (feature 26 <= 0.18)\n",
      "       Predict: 0.9579617834394905\n",
      "      Else (feature 26 > 0.18)\n",
      "       Predict: 0.2222222222222222\n",
      "     Else (feature 45 > 0.12)\n",
      "      If (feature 52 <= 0.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 52 > 0.0)\n",
      "       Predict: 0.7142857142857143\n",
      "    Else (feature 24 > 0.09)\n",
      "     If (feature 51 <= 0.49)\n",
      "      If (feature 6 <= 0.0)\n",
      "       Predict: 0.04878048780487805\n",
      "      Else (feature 6 > 0.0)\n",
      "       Predict: 1.0\n",
      "     Else (feature 51 > 0.49)\n",
      "      Predict: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawRDD: org.apache.spark.rdd.RDD[String] = files/spambase.data MapPartitionsRDD[46] at textFile at <console>:55\n",
       "dataDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n",
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "featureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_89d043b16003\n",
       "dt: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_b605f8eb0a48\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_a8294a84635b\n",
       "dtModel: org.apache.spark.ml.PipelineModel = pipeline_a8294a84635b\n",
       "predictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 2 more fields]\n",
       "testMSE: Double..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read Data\n",
    "val rawRDD = sparkSession.sparkContext.textFile(\"files/spambase.data\", 20)\n",
    "val dataDF = rawRDD.map(line => line.split(',').map(_.toDouble)).\n",
    "                    map(x => (x(57), org.apache.spark.ml.linalg.Vectors.dense(x.take(57)))).\n",
    "                    toDF(\"label\", \"features\")\n",
    "\n",
    "val Array(trainingData, testData) = dataDF.randomSplit(Array(0.7, 0.3), seed = 123)\n",
    "\n",
    "// Build pipeline about decision tree classification\n",
    "val featureIndexer = new VectorIndexer().\n",
    "  setInputCol(\"features\").\n",
    "  setOutputCol(\"indexedFeatures\").\n",
    "  setMaxCategories(10). // features with > 10 distinct values are treated as continuous.\n",
    "  fit(dataDF)\n",
    "\n",
    "val dt = new DecisionTreeRegressor().\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"indexedFeatures\").\n",
    "  setImpurity(\"variance\").\n",
    "  setMaxBins(32).\n",
    "  setMaxDepth(5)\n",
    "\n",
    "\n",
    "val pipeline = new Pipeline().\n",
    "  setStages(Array(featureIndexer, dt))\n",
    "\n",
    "val dtModel = pipeline.fit(trainingData)\n",
    "val predictions = dtModel.transform(testData)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val testMSE = predictions.select(\"label\", \"prediction\").\n",
    "    map{ case Row(v: Double, p: Double) => math.pow(v - p, 2) }.\n",
    "    rdd.mean()\n",
    "println(\"Test Mean Squared Error = \" + testMSE)\n",
    "println(\"Learned regression tree model:\\n\" + dtModel.stages(1).asInstanceOf[DecisionTreeRegressionModel].toDebugString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
